{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal\n",
    "\n",
    "Train a model to predict fraudulent credit card transactions\n",
    "\n",
    "### Tasks\n",
    "1. Load [data from Kaggle](https://www.kaggle.com/dalpozz/creditcardfraud/downloads/creditcardfraud.zip) and do exploratory data analysis to understand our data\n",
    "2. Clean data and prepare it in X (2-d array), y (1-d array) format for modeling\n",
    "3. Select algorithm and train model\n",
    "4. Evaluate model\n",
    "5. Tune/improve model\n",
    "6. Use model to predict fraudulent transactions\n",
    "\n",
    "### Approach\n",
    "1. Sample the data so as to reduce the skew\n",
    "2. Start with a logistic regression classifier\n",
    "3. Perform classifications model using other classification algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import imblearn\n",
    "\n",
    "%matplotlib inline\n",
    "pd.options.display.max_columns = 40\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.991390</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>1.468177</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>0.489095</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.635558</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.717293</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>2.345865</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>-0.121359</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>0.507757</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.631418</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>1.965775</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>1.345852</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>0.175121</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>-0.038195</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10       V11       V12       V13       V14  \\\n",
       "0  0.098698  0.363787  0.090794 -0.551600 -0.617801 -0.991390 -0.311169   \n",
       "1  0.085102 -0.255425 -0.166974  1.612727  1.065235  0.489095 -0.143772   \n",
       "2  0.247676 -1.514654  0.207643  0.624501  0.066084  0.717293 -0.165946   \n",
       "3  0.377436 -1.387024 -0.054952 -0.226487  0.178228  0.507757 -0.287924   \n",
       "4 -0.270533  0.817739  0.753074 -0.822843  0.538196  1.345852 -1.119670   \n",
       "\n",
       "        V15       V16       V17       V18       V19       V20       V21  \\\n",
       "0  1.468177 -0.470401  0.207971  0.025791  0.403993  0.251412 -0.018307   \n",
       "1  0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083 -0.225775   \n",
       "2  2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  0.247998   \n",
       "3 -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038 -0.108300   \n",
       "4  0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542 -0.009431   \n",
       "\n",
       "        V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "\n",
       "   Amount  Class  \n",
       "0  149.62      0  \n",
       "1    2.69      0  \n",
       "2  378.66      0  \n",
       "3  123.50      0  \n",
       "4   69.99      0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>1.165980e-15</td>\n",
       "      <td>3.416908e-16</td>\n",
       "      <td>-1.373150e-15</td>\n",
       "      <td>2.086869e-15</td>\n",
       "      <td>9.604066e-16</td>\n",
       "      <td>1.490107e-15</td>\n",
       "      <td>-5.556467e-16</td>\n",
       "      <td>1.177556e-16</td>\n",
       "      <td>-2.406455e-15</td>\n",
       "      <td>2.239751e-15</td>\n",
       "      <td>1.673327e-15</td>\n",
       "      <td>-1.254995e-15</td>\n",
       "      <td>8.176030e-16</td>\n",
       "      <td>1.206296e-15</td>\n",
       "      <td>4.913003e-15</td>\n",
       "      <td>1.437666e-15</td>\n",
       "      <td>-3.800113e-16</td>\n",
       "      <td>9.572133e-16</td>\n",
       "      <td>1.039817e-15</td>\n",
       "      <td>6.406703e-16</td>\n",
       "      <td>1.656562e-16</td>\n",
       "      <td>-3.444850e-16</td>\n",
       "      <td>2.578648e-16</td>\n",
       "      <td>4.471968e-15</td>\n",
       "      <td>5.340915e-16</td>\n",
       "      <td>1.687098e-15</td>\n",
       "      <td>-3.666453e-16</td>\n",
       "      <td>-1.220404e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>1.088850e+00</td>\n",
       "      <td>1.020713e+00</td>\n",
       "      <td>9.992014e-01</td>\n",
       "      <td>9.952742e-01</td>\n",
       "      <td>9.585956e-01</td>\n",
       "      <td>9.153160e-01</td>\n",
       "      <td>8.762529e-01</td>\n",
       "      <td>8.493371e-01</td>\n",
       "      <td>8.381762e-01</td>\n",
       "      <td>8.140405e-01</td>\n",
       "      <td>7.709250e-01</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>-2.458826e+01</td>\n",
       "      <td>-4.797473e+00</td>\n",
       "      <td>-1.868371e+01</td>\n",
       "      <td>-5.791881e+00</td>\n",
       "      <td>-1.921433e+01</td>\n",
       "      <td>-4.498945e+00</td>\n",
       "      <td>-1.412985e+01</td>\n",
       "      <td>-2.516280e+01</td>\n",
       "      <td>-9.498746e+00</td>\n",
       "      <td>-7.213527e+00</td>\n",
       "      <td>-5.449772e+01</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>-5.354257e-01</td>\n",
       "      <td>-7.624942e-01</td>\n",
       "      <td>-4.055715e-01</td>\n",
       "      <td>-6.485393e-01</td>\n",
       "      <td>-4.255740e-01</td>\n",
       "      <td>-5.828843e-01</td>\n",
       "      <td>-4.680368e-01</td>\n",
       "      <td>-4.837483e-01</td>\n",
       "      <td>-4.988498e-01</td>\n",
       "      <td>-4.562989e-01</td>\n",
       "      <td>-2.117214e-01</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>-9.291738e-02</td>\n",
       "      <td>-3.275735e-02</td>\n",
       "      <td>1.400326e-01</td>\n",
       "      <td>-1.356806e-02</td>\n",
       "      <td>5.060132e-02</td>\n",
       "      <td>4.807155e-02</td>\n",
       "      <td>6.641332e-02</td>\n",
       "      <td>-6.567575e-02</td>\n",
       "      <td>-3.636312e-03</td>\n",
       "      <td>3.734823e-03</td>\n",
       "      <td>-6.248109e-02</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>4.539234e-01</td>\n",
       "      <td>7.395934e-01</td>\n",
       "      <td>6.182380e-01</td>\n",
       "      <td>6.625050e-01</td>\n",
       "      <td>4.931498e-01</td>\n",
       "      <td>6.488208e-01</td>\n",
       "      <td>5.232963e-01</td>\n",
       "      <td>3.996750e-01</td>\n",
       "      <td>5.008067e-01</td>\n",
       "      <td>4.589494e-01</td>\n",
       "      <td>1.330408e-01</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>2.374514e+01</td>\n",
       "      <td>1.201891e+01</td>\n",
       "      <td>7.848392e+00</td>\n",
       "      <td>7.126883e+00</td>\n",
       "      <td>1.052677e+01</td>\n",
       "      <td>8.877742e+00</td>\n",
       "      <td>1.731511e+01</td>\n",
       "      <td>9.253526e+00</td>\n",
       "      <td>5.041069e+00</td>\n",
       "      <td>5.591971e+00</td>\n",
       "      <td>3.942090e+01</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  1.165980e-15  3.416908e-16 -1.373150e-15  2.086869e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   9.604066e-16  1.490107e-15 -5.556467e-16  1.177556e-16 -2.406455e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "                V10           V11           V12           V13           V14  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   2.239751e-15  1.673327e-15 -1.254995e-15  8.176030e-16  1.206296e-15   \n",
       "std    1.088850e+00  1.020713e+00  9.992014e-01  9.952742e-01  9.585956e-01   \n",
       "min   -2.458826e+01 -4.797473e+00 -1.868371e+01 -5.791881e+00 -1.921433e+01   \n",
       "25%   -5.354257e-01 -7.624942e-01 -4.055715e-01 -6.485393e-01 -4.255740e-01   \n",
       "50%   -9.291738e-02 -3.275735e-02  1.400326e-01 -1.356806e-02  5.060132e-02   \n",
       "75%    4.539234e-01  7.395934e-01  6.182380e-01  6.625050e-01  4.931498e-01   \n",
       "max    2.374514e+01  1.201891e+01  7.848392e+00  7.126883e+00  1.052677e+01   \n",
       "\n",
       "                V15           V16           V17           V18           V19  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   4.913003e-15  1.437666e-15 -3.800113e-16  9.572133e-16  1.039817e-15   \n",
       "std    9.153160e-01  8.762529e-01  8.493371e-01  8.381762e-01  8.140405e-01   \n",
       "min   -4.498945e+00 -1.412985e+01 -2.516280e+01 -9.498746e+00 -7.213527e+00   \n",
       "25%   -5.828843e-01 -4.680368e-01 -4.837483e-01 -4.988498e-01 -4.562989e-01   \n",
       "50%    4.807155e-02  6.641332e-02 -6.567575e-02 -3.636312e-03  3.734823e-03   \n",
       "75%    6.488208e-01  5.232963e-01  3.996750e-01  5.008067e-01  4.589494e-01   \n",
       "max    8.877742e+00  1.731511e+01  9.253526e+00  5.041069e+00  5.591971e+00   \n",
       "\n",
       "                V20           V21           V22           V23           V24  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   6.406703e-16  1.656562e-16 -3.444850e-16  2.578648e-16  4.471968e-15   \n",
       "std    7.709250e-01  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min   -5.449772e+01 -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%   -2.117214e-01 -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%   -6.248109e-02 -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    1.330408e-01  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    3.942090e+01  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   5.340915e-16  1.687098e-15 -3.666453e-16 -1.220404e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(df['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.998269524998681\n"
     ]
    }
   ],
   "source": [
    "non_fraud_percentage = (284315-492)/284315.0\n",
    "print(non_fraud_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a highly unbalanced dataset, and this will make it hard to train our model to detect fraud.\n",
    "If we wrote a function to always predict 0 (y=not_fraud), we would be correct **99.8%** of the time, but would not have detected any of the fraud cases.\n",
    "\n",
    "To deal with this, we have 3 options:\n",
    "1. **Weighting**: Assign the under-represented class a higher weight. However, this is unlikely to be effective given the significant skew in the dataset.\n",
    "2. **Thresholding**: Override the model's `.predict()` method to classify something as 0 or 1 based on a probability threshold (e.g. 0.90), rather than the probability with the higher value (e.g. 0.50001)\n",
    "3. **Sampling**: For each training set, sample it in such a way that the instances of 0 and 1 are roughly equal\n",
    "\n",
    "[Read more](https://stackoverflow.com/questions/26221312/dealing-with-the-class-imbalance-in-binary-classification/26244744#26244744)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing our data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df.ix[:, df.columns != 'Class']\n",
    "y = df.ix[:, df.columns == 'Class'].values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`.ravel()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ravel.html) is a method that helps us convert y (which is originally a column-vector) to a 1-dimensional array, so that scikit-learn won't throw a DataConversionWarning. The code will work without transforming it with `.values.ravel()` as well, but we'll have a warning message, which is not so nice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Split our data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 1: Logistic regression model with no sampling or thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_1 = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set score: 0.999078\n",
      "test set score:     0.999087\n"
     ]
    }
   ],
   "source": [
    "# 1. .score()\n",
    "train_score_1 = model_1.score(X_train, y_train)\n",
    "test_score_1 = model_1.score(X_test, y_test)\n",
    "\n",
    "print(\"training set score: %f\" % train_score_1)\n",
    "print(\"test set score:     %f\" % test_score_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expected_1 = y\n",
    "predicted_1 = model_1.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[284237     78]\n",
      " [   184    308]]\n"
     ]
    }
   ],
   "source": [
    "# 2. .confusion_matrix()\n",
    "print(metrics.confusion_matrix(expected_1, predicted_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the 2nd nested array (`[false_positives, true_positives]`), we see that we've correctly predicted **322** fraudulent transactions, and we've misclassified **170** fraudulent transactions as non-fraudulent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00    284315\n",
      "          1       0.80      0.63      0.70       492\n",
      "\n",
      "avg / total       1.00      1.00      1.00    284807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. .classification_report()\n",
    "print(metrics.classification_report(expected_1, predicted_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the precision score, we can see that **74% of our predictions of y=1 (fraud) were were**.\n",
    "\n",
    "Looking at the recall score, we can see that only **65% of the fraudulent cases in reality were correctly classified**.\n",
    "\n",
    "Note: remember our helpful mnemonic:\n",
    "- **pre**cision: a measure of our accuracy with our **pre**dictions as the baseline\n",
    "- **re**call: a measure of our accuracy with the **re**ality as the baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 2 (Logistic regression model with sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the accuracy of the model, we can undersample the data such that the proportion of cases of y=0 and y=1 are 50-50, instead of 99.8-0.2.\n",
    "\n",
    "**`imblearn`** (imbalanced_learn) is a nice library that has methods for doing this undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of X and y: 984 984\n",
      "Count of y values: Counter({1: 492, 0: 492})\n"
     ]
    }
   ],
   "source": [
    "rus = RandomUnderSampler(return_indices=True)\n",
    "X_undersampled, y_undersampled, idx_resampled = rus.fit_sample(X, y)\n",
    "print('length of X and y:', len(X_undersampled), len(y_undersampled))\n",
    "print('Count of y values:', collections.Counter(y_undersampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_undersampled_25_percent_split, X_test_undersampled_25_percent_split, y_train_undersampled_25_percent_split,\\\n",
    "    y_test_undersampled_25_percent_split = train_test_split(X_undersampled, y_undersampled, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = LogisticRegression()\n",
    "model_2.fit(X_train_undersampled_25_percent_split, y_train_undersampled_25_percent_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[477  15]\n",
      " [ 39 453]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.97      0.95       492\n",
      "          1       0.97      0.92      0.94       492\n",
      "\n",
      "avg / total       0.95      0.95      0.95       984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expected = y_undersampled\n",
    "predicted = model_2.predict(X_undersampled)\n",
    "\n",
    "print(metrics.confusion_matrix(expected, predicted))\n",
    "print(metrics.classification_report(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 3 (Logistic regression model with GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator: LogisticRegression(C=0.1, class_weight={0: 1, 1: 2}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "Best score: 0.999180730788\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_model = LogisticRegression()\n",
    "\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10],\n",
    "              'class_weight': [{\n",
    "                  0: 1, \n",
    "                  1: 2\n",
    "              },\n",
    "              {\n",
    "                  0: 1, \n",
    "                  1: 1.2\n",
    "              },\n",
    "              {\n",
    "                  0: 1, \n",
    "                  1: 1.4\n",
    "              }\n",
    "              ]}\n",
    "\n",
    "model_3 = GridSearchCV(estimator=logistic_regression_model, param_grid=param_grid, cv=5)\n",
    "model_3.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best estimator:\", model_3.best_estimator_)\n",
    "print(\"Best score:\", model_3.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[284255     60]\n",
      " [   124    368]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00    284315\n",
      "          1       0.86      0.75      0.80       492\n",
      "\n",
      "avg / total       1.00      1.00      1.00    284807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expected_3 = y\n",
    "predicted_3 = model_3.predict(X)\n",
    "\n",
    "print(metrics.confusion_matrix(expected_3, predicted_3))\n",
    "print(metrics.classification_report(expected_3, predicted_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 4: (Logistic regression model with undersampling and GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator: LogisticRegression(C=0.1, class_weight={0: 1, 1: 1.4}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "Best score: 0.937669376694\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_model = LogisticRegression()\n",
    "\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10],\n",
    "              'class_weight': [{\n",
    "                  0: 1, \n",
    "                  1: 2\n",
    "              },\n",
    "              {\n",
    "                  0: 1, \n",
    "                  1: 1.2\n",
    "              },\n",
    "              {\n",
    "                  0: 1, \n",
    "                  1: 1.4\n",
    "              }]}\n",
    "\n",
    "model_4 = GridSearchCV(estimator=logistic_regression_model, param_grid=param_grid, cv=5)\n",
    "model_4.fit(X_train_undersampled_25_percent_split, y_train_undersampled_25_percent_split)\n",
    "\n",
    "print(\"Best estimator:\", model_4.best_estimator_)\n",
    "print(\"Best score:\", model_4.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[275363   8952]\n",
      " [    55    437]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.97      0.98    284315\n",
      "          1       0.05      0.89      0.09       492\n",
      "\n",
      "avg / total       1.00      0.97      0.98    284807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expected_4 = y\n",
    "predicted_4 = model_4.predict(X)\n",
    "\n",
    "print(metrics.confusion_matrix(expected_4, predicted_4))\n",
    "print(metrics.classification_report(expected_4, predicted_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 5: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5 = RandomForestClassifier()\n",
    "model_5.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[284310      5]\n",
      " [    46    446]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00    284315\n",
      "          1       0.99      0.91      0.95       492\n",
      "\n",
      "avg / total       1.00      1.00      1.00    284807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expected_5 = y\n",
    "predicted_5 = model_5.predict(X)\n",
    "\n",
    "print(metrics.confusion_matrix(expected_5, predicted_5))\n",
    "print(metrics.classification_report(expected_5, predicted_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01051503,  0.01315391,  0.0093916 ,  0.01412618,  0.03471292,\n",
       "        0.01020134,  0.01295551,  0.02506841,  0.01235664,  0.02527668,\n",
       "        0.0853672 ,  0.11987168,  0.14544408,  0.00936543,  0.1150287 ,\n",
       "        0.01372437,  0.08678474,  0.0987931 ,  0.04612033,  0.01017366,\n",
       "        0.01341121,  0.00994655,  0.00826253,  0.00811489,  0.00666897,\n",
       "        0.01155721,  0.01254294,  0.01446959,  0.00900062,  0.00759398])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?plt.xticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x119c78f98>,\n",
       "  <matplotlib.axis.XTick at 0x1184cc828>,\n",
       "  <matplotlib.axis.XTick at 0x1183930f0>,\n",
       "  <matplotlib.axis.XTick at 0x119cb7080>,\n",
       "  <matplotlib.axis.XTick at 0x119cb7a58>,\n",
       "  <matplotlib.axis.XTick at 0x119cbe470>,\n",
       "  <matplotlib.axis.XTick at 0x119cbee48>,\n",
       "  <matplotlib.axis.XTick at 0x119cc5860>,\n",
       "  <matplotlib.axis.XTick at 0x119ccb278>,\n",
       "  <matplotlib.axis.XTick at 0x119ccbc50>,\n",
       "  <matplotlib.axis.XTick at 0x119cd0668>,\n",
       "  <matplotlib.axis.XTick at 0x119cd9080>,\n",
       "  <matplotlib.axis.XTick at 0x119cd9a58>,\n",
       "  <matplotlib.axis.XTick at 0x119cdf470>,\n",
       "  <matplotlib.axis.XTick at 0x119cdfe48>,\n",
       "  <matplotlib.axis.XTick at 0x119ce4860>,\n",
       "  <matplotlib.axis.XTick at 0x119ced278>,\n",
       "  <matplotlib.axis.XTick at 0x119cedc50>,\n",
       "  <matplotlib.axis.XTick at 0x119cf2668>,\n",
       "  <matplotlib.axis.XTick at 0x119cf9080>,\n",
       "  <matplotlib.axis.XTick at 0x119cf9a58>,\n",
       "  <matplotlib.axis.XTick at 0x119d01470>,\n",
       "  <matplotlib.axis.XTick at 0x119d01e48>,\n",
       "  <matplotlib.axis.XTick at 0x119d08860>,\n",
       "  <matplotlib.axis.XTick at 0x119d0d278>,\n",
       "  <matplotlib.axis.XTick at 0x119d0dc50>,\n",
       "  <matplotlib.axis.XTick at 0x119d15668>,\n",
       "  <matplotlib.axis.XTick at 0x119d1b080>,\n",
       "  <matplotlib.axis.XTick at 0x119d1ba58>,\n",
       "  <matplotlib.axis.XTick at 0x119d22470>,\n",
       "  <matplotlib.axis.XTick at 0x119d22e48>,\n",
       "  <matplotlib.axis.XTick at 0x119d29860>],\n",
       " <a list of 32 Text xticklabel objects>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD/CAYAAAAZg9YLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1184a1128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model_5.feature_importances_, 'o')\n",
    "plt.xticks(range(32), df.index, rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 6: Random Forest (with undersampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_6 = RandomForestClassifier()\n",
    "model_6.fit(X_train_undersampled_25_percent_split, y_train_undersampled_25_percent_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[276452   7863]\n",
      " [    14    478]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.97      0.99    284315\n",
      "          1       0.06      0.97      0.11       492\n",
      "\n",
      "avg / total       1.00      0.97      0.98    284807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expected_6 = y\n",
    "predicted_6 = model_6.predict(X)\n",
    "\n",
    "print(metrics.confusion_matrix(expected_6, predicted_6))\n",
    "print(metrics.classification_report(expected_6, predicted_6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 7: Random Forest (with undersampling, and 40% train_test_split ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that we're not overfitting, let's try it with 40% train_test_split ratio (i.e. 40% of the data will be held off for testing/validating), instead of the default ratio of 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_undersampled_40_percent_split, X_test_undersampled_40_percent_split, y_train_undersampled_40_percent_split,\\\n",
    "    y_test_undersampled_40_percent_split = train_test_split(X_undersampled,\n",
    "                                                            y_undersampled,\n",
    "                                                            test_size=0.4,\n",
    "                                                            random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_7 = RandomForestClassifier()\n",
    "model_7.fit(X_train_undersampled_40_percent_split, y_train_undersampled_40_percent_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[284310      5]\n",
      " [    54    438]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00    284315\n",
      "          1       0.99      0.89      0.94       492\n",
      "\n",
      "avg / total       1.00      1.00      1.00    284807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(expected_7, predicted_7))\n",
    "print(metrics.classification_report(expected_7, predicted_7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our recall score has dropped from 0.97 to **0.89**, which confirms our suspicion that our earlier score of 0.97 was due to overfitting! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[275332   8983]\n",
      " [    23    469]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.97      0.98    284315\n",
      "          1       0.05      0.95      0.09       492\n",
      "\n",
      "avg / total       1.00      0.97      0.98    284807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TO DELETE\n",
    "# results based on training with 0.4 train_test_split ratio (i.e. 40% of data is reserved for testing)\n",
    "print(metrics.confusion_matrix(expected_7, predicted_7))\n",
    "print(metrics.classification_report(expected_7, predicted_7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 8: Random Forest (with undersampling, and 40% train_test_split ratio, and optimization with GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_forest_classifier_model = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know which params we can tune, you can use the `.get_params` property. As for what values to put in, this will require some reading and general googling :-)\n",
    "\n",
    "Generally, random forests are tuned by tweaking the following hyperparameters:\n",
    "- max_features\n",
    "- n_estimators\n",
    "- min_samples_leaf\n",
    "- class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_classifier_model.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator: RandomForestClassifier(bootstrap=True, class_weight={0: 1, 1: 2},\n",
      "            criterion='gini', max_depth=None, max_features='log2',\n",
      "            max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "Best score: 0.942372881356\n"
     ]
    }
   ],
   "source": [
    "random_forest_classifier_model = RandomForestClassifier()\n",
    "\n",
    "param_grid = {'max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "              'n_estimators': [1, 2, 4, 8, 10, 20, 30, 50],\n",
    "              'min_samples_leaf': [1,5,10,50],\n",
    "              'class_weight': [{\n",
    "                  0: 1, \n",
    "                  1: 1\n",
    "              },\n",
    "              {\n",
    "                  0: 1, \n",
    "                  1: 1.5\n",
    "              },\n",
    "              {\n",
    "                  0: 1, \n",
    "                  1: 2\n",
    "              },\n",
    "              {\n",
    "                  0: 1, \n",
    "                  1: 2.5\n",
    "              }\n",
    "              ]}\n",
    "\n",
    "model_8 = GridSearchCV(estimator=random_forest_classifier_model, \n",
    "                       param_grid=param_grid, cv=5)\n",
    "model_8.fit(X_train_undersampled_40_percent_split, y_train_undersampled_40_percent_split)\n",
    "\n",
    "print(\"Best estimator:\", model_8.best_estimator_)\n",
    "print(\"Best score:\", model_8.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[277980   6335]\n",
      " [    20    472]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.98      0.99    284315\n",
      "          1       0.07      0.96      0.13       492\n",
      "\n",
      "avg / total       1.00      0.98      0.99    284807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expected_8 = y\n",
    "predicted_8 = model_8.predict(X)\n",
    "\n",
    "print(metrics.confusion_matrix(expected_8, predicted_8))\n",
    "print(metrics.classification_report(expected_8, predicted_8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "1. LogisticRegression models are a great starting point for building classification models\n",
    "2. RandomForestClassifier models performs better than LogisticRegression out of the box, without any tuning/optimisation\n",
    "3. RandomForestClassifier models outperform LogisticRegression models when tuned\n",
    "4. Undersampling is a useful technique for training models with highly skewed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
