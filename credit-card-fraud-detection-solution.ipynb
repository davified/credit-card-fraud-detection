{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal\n",
    "\n",
    "Train a model to predict fraudulent credit card transactions\n",
    "\n",
    "### Tasks\n",
    "1. Load [data from Kaggle](https://www.kaggle.com/dalpozz/creditcardfraud/downloads/creditcardfraud.zip) and do exploratory data analysis to understand our data\n",
    "2. Clean data and prepare it in X (2-d array), y (1-d array) format for modeling\n",
    "3. Select algorithm and train model\n",
    "4. Evaluate model\n",
    "5. Tune/improve model\n",
    "6. Use model to predict fraudulent transactions\n",
    "\n",
    "### Approach\n",
    "1. Sample the data so as to reduce the skew\n",
    "2. Start with a logistic regression classifier\n",
    "3. Perform classifications model using other classification algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import imblearn\n",
    "\n",
    "%matplotlib inline\n",
    "pd.options.display.max_columns = 40\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.991390</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>1.468177</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>0.489095</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.635558</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.717293</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>2.345865</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>-0.121359</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>0.507757</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.631418</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>1.965775</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>1.345852</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>0.175121</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>-0.038195</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10       V11       V12       V13       V14  \\\n",
       "0  0.098698  0.363787  0.090794 -0.551600 -0.617801 -0.991390 -0.311169   \n",
       "1  0.085102 -0.255425 -0.166974  1.612727  1.065235  0.489095 -0.143772   \n",
       "2  0.247676 -1.514654  0.207643  0.624501  0.066084  0.717293 -0.165946   \n",
       "3  0.377436 -1.387024 -0.054952 -0.226487  0.178228  0.507757 -0.287924   \n",
       "4 -0.270533  0.817739  0.753074 -0.822843  0.538196  1.345852 -1.119670   \n",
       "\n",
       "        V15       V16       V17       V18       V19       V20       V21  \\\n",
       "0  1.468177 -0.470401  0.207971  0.025791  0.403993  0.251412 -0.018307   \n",
       "1  0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083 -0.225775   \n",
       "2  2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  0.247998   \n",
       "3 -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038 -0.108300   \n",
       "4  0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542 -0.009431   \n",
       "\n",
       "        V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "\n",
       "   Amount  Class  \n",
       "0  149.62      0  \n",
       "1    2.69      0  \n",
       "2  378.66      0  \n",
       "3  123.50      0  \n",
       "4   69.99      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>1.165980e-15</td>\n",
       "      <td>3.416908e-16</td>\n",
       "      <td>-1.373150e-15</td>\n",
       "      <td>2.086869e-15</td>\n",
       "      <td>9.604066e-16</td>\n",
       "      <td>1.490107e-15</td>\n",
       "      <td>-5.556467e-16</td>\n",
       "      <td>1.177556e-16</td>\n",
       "      <td>-2.406455e-15</td>\n",
       "      <td>2.239751e-15</td>\n",
       "      <td>1.673327e-15</td>\n",
       "      <td>-1.254995e-15</td>\n",
       "      <td>8.176030e-16</td>\n",
       "      <td>1.206296e-15</td>\n",
       "      <td>4.913003e-15</td>\n",
       "      <td>1.437666e-15</td>\n",
       "      <td>-3.800113e-16</td>\n",
       "      <td>9.572133e-16</td>\n",
       "      <td>1.039817e-15</td>\n",
       "      <td>6.406703e-16</td>\n",
       "      <td>1.656562e-16</td>\n",
       "      <td>-3.444850e-16</td>\n",
       "      <td>2.578648e-16</td>\n",
       "      <td>4.471968e-15</td>\n",
       "      <td>5.340915e-16</td>\n",
       "      <td>1.687098e-15</td>\n",
       "      <td>-3.666453e-16</td>\n",
       "      <td>-1.220404e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>1.088850e+00</td>\n",
       "      <td>1.020713e+00</td>\n",
       "      <td>9.992014e-01</td>\n",
       "      <td>9.952742e-01</td>\n",
       "      <td>9.585956e-01</td>\n",
       "      <td>9.153160e-01</td>\n",
       "      <td>8.762529e-01</td>\n",
       "      <td>8.493371e-01</td>\n",
       "      <td>8.381762e-01</td>\n",
       "      <td>8.140405e-01</td>\n",
       "      <td>7.709250e-01</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>-2.458826e+01</td>\n",
       "      <td>-4.797473e+00</td>\n",
       "      <td>-1.868371e+01</td>\n",
       "      <td>-5.791881e+00</td>\n",
       "      <td>-1.921433e+01</td>\n",
       "      <td>-4.498945e+00</td>\n",
       "      <td>-1.412985e+01</td>\n",
       "      <td>-2.516280e+01</td>\n",
       "      <td>-9.498746e+00</td>\n",
       "      <td>-7.213527e+00</td>\n",
       "      <td>-5.449772e+01</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>-5.354257e-01</td>\n",
       "      <td>-7.624942e-01</td>\n",
       "      <td>-4.055715e-01</td>\n",
       "      <td>-6.485393e-01</td>\n",
       "      <td>-4.255740e-01</td>\n",
       "      <td>-5.828843e-01</td>\n",
       "      <td>-4.680368e-01</td>\n",
       "      <td>-4.837483e-01</td>\n",
       "      <td>-4.988498e-01</td>\n",
       "      <td>-4.562989e-01</td>\n",
       "      <td>-2.117214e-01</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>-9.291738e-02</td>\n",
       "      <td>-3.275735e-02</td>\n",
       "      <td>1.400326e-01</td>\n",
       "      <td>-1.356806e-02</td>\n",
       "      <td>5.060132e-02</td>\n",
       "      <td>4.807155e-02</td>\n",
       "      <td>6.641332e-02</td>\n",
       "      <td>-6.567575e-02</td>\n",
       "      <td>-3.636312e-03</td>\n",
       "      <td>3.734823e-03</td>\n",
       "      <td>-6.248109e-02</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>4.539234e-01</td>\n",
       "      <td>7.395934e-01</td>\n",
       "      <td>6.182380e-01</td>\n",
       "      <td>6.625050e-01</td>\n",
       "      <td>4.931498e-01</td>\n",
       "      <td>6.488208e-01</td>\n",
       "      <td>5.232963e-01</td>\n",
       "      <td>3.996750e-01</td>\n",
       "      <td>5.008067e-01</td>\n",
       "      <td>4.589494e-01</td>\n",
       "      <td>1.330408e-01</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>2.374514e+01</td>\n",
       "      <td>1.201891e+01</td>\n",
       "      <td>7.848392e+00</td>\n",
       "      <td>7.126883e+00</td>\n",
       "      <td>1.052677e+01</td>\n",
       "      <td>8.877742e+00</td>\n",
       "      <td>1.731511e+01</td>\n",
       "      <td>9.253526e+00</td>\n",
       "      <td>5.041069e+00</td>\n",
       "      <td>5.591971e+00</td>\n",
       "      <td>3.942090e+01</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  1.165980e-15  3.416908e-16 -1.373150e-15  2.086869e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   9.604066e-16  1.490107e-15 -5.556467e-16  1.177556e-16 -2.406455e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "                V10           V11           V12           V13           V14  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   2.239751e-15  1.673327e-15 -1.254995e-15  8.176030e-16  1.206296e-15   \n",
       "std    1.088850e+00  1.020713e+00  9.992014e-01  9.952742e-01  9.585956e-01   \n",
       "min   -2.458826e+01 -4.797473e+00 -1.868371e+01 -5.791881e+00 -1.921433e+01   \n",
       "25%   -5.354257e-01 -7.624942e-01 -4.055715e-01 -6.485393e-01 -4.255740e-01   \n",
       "50%   -9.291738e-02 -3.275735e-02  1.400326e-01 -1.356806e-02  5.060132e-02   \n",
       "75%    4.539234e-01  7.395934e-01  6.182380e-01  6.625050e-01  4.931498e-01   \n",
       "max    2.374514e+01  1.201891e+01  7.848392e+00  7.126883e+00  1.052677e+01   \n",
       "\n",
       "                V15           V16           V17           V18           V19  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   4.913003e-15  1.437666e-15 -3.800113e-16  9.572133e-16  1.039817e-15   \n",
       "std    9.153160e-01  8.762529e-01  8.493371e-01  8.381762e-01  8.140405e-01   \n",
       "min   -4.498945e+00 -1.412985e+01 -2.516280e+01 -9.498746e+00 -7.213527e+00   \n",
       "25%   -5.828843e-01 -4.680368e-01 -4.837483e-01 -4.988498e-01 -4.562989e-01   \n",
       "50%    4.807155e-02  6.641332e-02 -6.567575e-02 -3.636312e-03  3.734823e-03   \n",
       "75%    6.488208e-01  5.232963e-01  3.996750e-01  5.008067e-01  4.589494e-01   \n",
       "max    8.877742e+00  1.731511e+01  9.253526e+00  5.041069e+00  5.591971e+00   \n",
       "\n",
       "                V20           V21           V22           V23           V24  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   6.406703e-16  1.656562e-16 -3.444850e-16  2.578648e-16  4.471968e-15   \n",
       "std    7.709250e-01  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min   -5.449772e+01 -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%   -2.117214e-01 -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%   -6.248109e-02 -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    1.330408e-01  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    3.942090e+01  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   5.340915e-16  1.687098e-15 -3.666453e-16 -1.220404e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(df['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.998269524998681\n"
     ]
    }
   ],
   "source": [
    "non_fraud_percentage = (284315-492)/284315.0\n",
    "print(non_fraud_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a highly unbalanced dataset, and this will make it hard to train our model to detect fraud.\n",
    "If we wrote a function to always predict 0 (y=not_fraud), we would be correct **99.8%** of the time, but would not have detected any of the fraud cases.\n",
    "\n",
    "To deal with this, we have 3 options:\n",
    "1. **Weighting**: Assign the under-represented class a higher weight. However, this is unlikely to be effective given the significant skew in the dataset.\n",
    "2. **Thresholding**: Override the model's `.predict()` method to classify something as 0 or 1 based on a probability threshold (e.g. 0.90), rather than the probability with the higher value (e.g. 0.50001)\n",
    "3. **Sampling**: For each training set, sample it in such a way that the instances of 0 and 1 are roughly equal\n",
    "\n",
    "[Read more](https://stackoverflow.com/questions/26221312/dealing-with-the-class-imbalance-in-binary-classification/26244744#26244744)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing our data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df.ix[:, df.columns != 'Class']\n",
    "y = df.ix[:, df.columns == 'Class'].values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`.ravel()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ravel.html) is a method that helps us convert y (which is originally a column-vector) to a 1-dimensional array, so that scikit-learn won't throw a DataConversionWarning. The code will work without transforming it with `.values.ravel()` as well, but we'll have a warning message, which is not so nice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Split our data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 1: Logistic regression model (with no sampling or thresholding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_1 = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set score: 0.999078\n",
      "test set score:     0.999087\n"
     ]
    }
   ],
   "source": [
    "# 1. .score()\n",
    "train_score_1 = model_1.score(X_train, y_train)\n",
    "test_score_1 = model_1.score(X_test, y_test)\n",
    "\n",
    "print(\"training set score: %f\" % train_score_1)\n",
    "print(\"test set score:     %f\" % test_score_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expected_1 = y\n",
    "predicted_1 = model_1.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[284237     78]\n",
      " [   184    308]]\n"
     ]
    }
   ],
   "source": [
    "# 2. .confusion_matrix()\n",
    "print(metrics.confusion_matrix(expected_1, predicted_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the 2nd nested array (`[false_positives, true_positives]`), we see that we've correctly predicted **322** fraudulent transactions, and we've misclassified **170** fraudulent transactions as non-fraudulent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00    284315\n",
      "          1       0.80      0.63      0.70       492\n",
      "\n",
      "avg / total       1.00      1.00      1.00    284807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. .classification_report()\n",
    "print(metrics.classification_report(expected_1, predicted_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the precision score, we can see that **74% of our predictions of y=1 (fraud) were were**.\n",
    "\n",
    "Looking at the recall score, we can see that only **65% of the fraudulent cases in reality were correctly classified**.\n",
    "\n",
    "Note: remember our helpful mnemonic:\n",
    "- **pre**cision: a measure of our accuracy with our **pre**dictions as the baseline\n",
    "- **re**call: a measure of our accuracy with the **re**ality as the baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 2: Logistic regression model (with undersampled data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the accuracy of the model, we can undersample the data such that the proportion of cases of y=0 and y=1 are 50-50, instead of 99.8-0.2.\n",
    "\n",
    "**`imblearn`** (imbalanced_learn) is a nice library that has methods for doing this undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of X and y: 984 984\n",
      "Count of y values: Counter({1: 492, 0: 492})\n"
     ]
    }
   ],
   "source": [
    "rus = RandomUnderSampler(return_indices=True)\n",
    "X_undersampled, y_undersampled, idx_resampled = rus.fit_sample(X, y)\n",
    "print('length of X and y:', len(X_undersampled), len(y_undersampled))\n",
    "print('Count of y values:', collections.Counter(y_undersampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_undersampled_25_percent_split, X_test_undersampled_25_percent_split, y_train_undersampled_25_percent_split,\\\n",
    "    y_test_undersampled_25_percent_split = train_test_split(X_undersampled, y_undersampled, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = LogisticRegression()\n",
    "model_2.fit(X_train_undersampled_25_percent_split, y_train_undersampled_25_percent_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[483   9]\n",
      " [ 38 454]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.98      0.95       492\n",
      "          1       0.98      0.92      0.95       492\n",
      "\n",
      "avg / total       0.95      0.95      0.95       984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expected = y_undersampled\n",
    "predicted = model_2.predict(X_undersampled)\n",
    "\n",
    "print(metrics.confusion_matrix(expected, predicted))\n",
    "print(metrics.classification_report(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 3: Logistic regression model (with GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator: LogisticRegression(C=0.1, class_weight={0: 1, 1: 2}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "Best score: 0.999166686173\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_model = LogisticRegression()\n",
    "\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10],\n",
    "              'class_weight': [{\n",
    "                  0: 1, \n",
    "                  1: 2\n",
    "              },\n",
    "              {\n",
    "                  0: 1, \n",
    "                  1: 1.2\n",
    "              },\n",
    "              {\n",
    "                  0: 1, \n",
    "                  1: 1.4\n",
    "              }\n",
    "              ]}\n",
    "\n",
    "model_3 = GridSearchCV(estimator=logistic_regression_model, param_grid=param_grid, cv=5)\n",
    "model_3.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best estimator:\", model_3.best_estimator_)\n",
    "print(\"Best score:\", model_3.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[284243     72]\n",
      " [   126    366]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00    284315\n",
      "          1       0.84      0.74      0.79       492\n",
      "\n",
      "avg / total       1.00      1.00      1.00    284807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expected_3 = y\n",
    "predicted_3 = model_3.predict(X)\n",
    "\n",
    "print(metrics.confusion_matrix(expected_3, predicted_3))\n",
    "print(metrics.classification_report(expected_3, predicted_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 4: Logistic regression model (with undersampled data and GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator: LogisticRegression(C=1, class_weight={0: 1, 1: 1.2}, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n",
      "Best score: 0.943089430894\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_model = LogisticRegression()\n",
    "\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10],\n",
    "              'class_weight': [{\n",
    "                  0: 1, \n",
    "                  1: 2\n",
    "              },\n",
    "              {\n",
    "                  0: 1, \n",
    "                  1: 1.2\n",
    "              },\n",
    "              {\n",
    "                  0: 1, \n",
    "                  1: 1.4\n",
    "              }]}\n",
    "\n",
    "model_4 = GridSearchCV(estimator=logistic_regression_model, param_grid=param_grid, cv=5)\n",
    "model_4.fit(X_train_undersampled_25_percent_split, y_train_undersampled_25_percent_split)\n",
    "\n",
    "print(\"Best estimator:\", model_4.best_estimator_)\n",
    "print(\"Best score:\", model_4.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[270880  13435]\n",
      " [    37    455]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.98    284315\n",
      "          1       0.03      0.92      0.06       492\n",
      "\n",
      "avg / total       1.00      0.95      0.97    284807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expected_4 = y\n",
    "predicted_4 = model_4.predict(X)\n",
    "\n",
    "print(metrics.confusion_matrix(expected_4, predicted_4))\n",
    "print(metrics.classification_report(expected_4, predicted_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 5: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5 = RandomForestClassifier(random_state=0)\n",
    "model_5.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[284307      8]\n",
      " [    47    445]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00    284315\n",
      "          1       0.98      0.90      0.94       492\n",
      "\n",
      "avg / total       1.00      1.00      1.00    284807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expected_5 = y\n",
    "predicted_5 = model_5.predict(X)\n",
    "\n",
    "print(metrics.confusion_matrix(expected_5, predicted_5))\n",
    "print(metrics.classification_report(expected_5, predicted_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus step: View/plot feature\\_importances\\_ in a random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01931921,  0.01453297,  0.01743606,  0.00990411,  0.01659406,\n",
       "        0.02401933,  0.00952278,  0.03290964,  0.01136424,  0.02613168,\n",
       "        0.07649404,  0.10074641,  0.13397464,  0.0090414 ,  0.10099308,\n",
       "        0.00855514,  0.10054981,  0.16083879,  0.01084574,  0.00842738,\n",
       "        0.01664699,  0.01079688,  0.00932942,  0.00475807,  0.01280383,\n",
       "        0.00511443,  0.02028118,  0.01151169,  0.00678298,  0.00977402])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEZCAYAAAB7HPUdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOOmbmZWIk76ZWYn8fwTgVOhMKcU2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1072a1be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model_5.feature_importances_, 'o')\n",
    "plt.xticks(range(32), df.columns.values, rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 6: Random Forest (with undersampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_6 = RandomForestClassifier()\n",
    "model_6.fit(X_train_undersampled_25_percent_split, y_train_undersampled_25_percent_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[276031   8284]\n",
      " [    10    482]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.97      0.99    284315\n",
      "          1       0.05      0.98      0.10       492\n",
      "\n",
      "avg / total       1.00      0.97      0.98    284807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expected_6 = y\n",
    "predicted_6 = model_6.predict(X)\n",
    "\n",
    "print(metrics.confusion_matrix(expected_6, predicted_6))\n",
    "print(metrics.classification_report(expected_6, predicted_6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 7: Random Forest (with undersampled data, and 40% train_test_split ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that we're not overfitting, let's try it with 40% train_test_split ratio (i.e. 40% of the data will be held off for testing/validating), instead of the default ratio of 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_undersampled_40_percent_split, X_test_undersampled_40_percent_split, y_train_undersampled_40_percent_split,\\\n",
    "    y_test_undersampled_40_percent_split = train_test_split(X_undersampled,\n",
    "                                                            y_undersampled,\n",
    "                                                            test_size=0.4,\n",
    "                                                            random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_7 = RandomForestClassifier(random_state=0)\n",
    "model_7.fit(X_train_undersampled_40_percent_split, y_train_undersampled_40_percent_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[187   8]\n",
      " [ 18 181]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.96      0.94       195\n",
      "          1       0.96      0.91      0.93       199\n",
      "\n",
      "avg / total       0.94      0.93      0.93       394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expected_7 = y_test_undersampled_40_percent_split\n",
    "predicted_7 = model_7.predict(X_test_undersampled_40_percent_split)\n",
    "\n",
    "print(metrics.confusion_matrix(expected_7, predicted_7))\n",
    "print(metrics.classification_report(expected_7, predicted_7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our recall score has dropped from 0.97 to **0.91**, which confirms our suspicion that our earlier score of 0.97 was due to overfitting! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 8: Random Forest (with undersampling, and 40% train_test_split ratio, and optimization with GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_forest_classifier_model = RandomForestClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know which params we can tune, you can use the `.get_params` property. As for what values to put in, this will require some reading and general googling :-)\n",
    "\n",
    "Generally, random forests are tuned by tweaking the following hyperparameters:\n",
    "- max_features\n",
    "- n_estimators\n",
    "- min_samples_leaf\n",
    "- class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_classifier_model.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator: RandomForestClassifier(bootstrap=True, class_weight={0: 1, 1: 2.5},\n",
      "            criterion='gini', max_depth=None, max_features='log2',\n",
      "            max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "            min_samples_leaf=5, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=1,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
      "Best score: 0.947457627119\n"
     ]
    }
   ],
   "source": [
    "random_forest_classifier_model = RandomForestClassifier(random_state=0)\n",
    "\n",
    "param_grid = {'max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "              'n_estimators': [1, 2, 4, 8, 10, 20, 30, 50],\n",
    "              'min_samples_leaf': [1,5,10,50],\n",
    "              'class_weight': [{\n",
    "                  0: 1, \n",
    "                  1: 1\n",
    "              },\n",
    "              {\n",
    "                  0: 1, \n",
    "                  1: 1.5\n",
    "              },\n",
    "              {\n",
    "                  0: 1, \n",
    "                  1: 2\n",
    "              },\n",
    "              {\n",
    "                  0: 1, \n",
    "                  1: 2.5\n",
    "              }\n",
    "              ]}\n",
    "\n",
    "model_8 = GridSearchCV(estimator=random_forest_classifier_model, \n",
    "                       param_grid=param_grid, cv=5)\n",
    "model_8.fit(X_train_undersampled_40_percent_split, y_train_undersampled_40_percent_split)\n",
    "\n",
    "print(\"Best estimator:\", model_8.best_estimator_)\n",
    "print(\"Best score:\", model_8.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[270172  14143]\n",
      " [    25    467]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.97    284315\n",
      "          1       0.03      0.95      0.06       492\n",
      "\n",
      "avg / total       1.00      0.95      0.97    284807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expected_8 = y\n",
    "predicted_8 = model_8.predict(X)\n",
    "\n",
    "print(metrics.confusion_matrix(expected_8, predicted_8))\n",
    "print(metrics.classification_report(expected_8, predicted_8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 9: Random Forest (with undersampling, and 40% train_test_split ratio, and optimization with RandomizedSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import randint as sp_randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False),\n",
       "          fit_params={}, iid=True, n_iter=30, n_jobs=-1,\n",
       "          param_distributions={'max_depth': [3, None], 'max_features': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 'bootstrap': [True, False], 'criterion': ['gini', 'entropy']},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score=True, scoring='recall', verbose=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_model = RandomForestClassifier(random_state=0)\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": list(range(1, 12)),\n",
    "              \"min_samples_split\": list(range(2, 12)),\n",
    "              \"min_samples_leaf\": list(range(1, 12)),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "model_9 = RandomizedSearchCV(rfc_model, \n",
    "                             n_iter=30, \n",
    "                             param_distributions=param_dist, \n",
    "                             scoring='recall', \n",
    "                             cv=5,\n",
    "                             n_jobs=-1)\n",
    "\n",
    "model_9.fit(X_train_undersampled_40_percent_split, y_train_undersampled_40_percent_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[272762  11553]\n",
      " [    20    472]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.96      0.98    284315\n",
      "          1       0.04      0.96      0.08       492\n",
      "\n",
      "avg / total       1.00      0.96      0.98    284807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expected_9 = y\n",
    "predicted_9 = model_9.predict(X)\n",
    "\n",
    "print(metrics.confusion_matrix(expected_9, predicted_9))\n",
    "print(metrics.classification_report(expected_9, predicted_9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "1. Wahoo! model_9 gave us a 0.96 recall rate! In other words, our model can **identify fraud with up to 96% accuracy**, even when only given 60% of the data as training data. We achieved this by using the following techniques:\n",
    "  - Resampling with `imblearn.under_sampling.RandomUnderSampler` to get a balanced training dataset with an equal number of fraud (492 cases) and non-fraud (also 492 cases).\n",
    "  - Randomized search cross validation\n",
    "  \n",
    "2. LogisticRegression models are a great starting point for building classification models\n",
    "3. RandomForestClassifier models performs better than LogisticRegression out of the box, even without any tuning/optimisation\n",
    "4. Undersampling is a useful technique for training models with highly skewed data\n",
    "5. GridSearchCV and RandomizedSearchCV allow us search the hyperparameter space to find the most optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
